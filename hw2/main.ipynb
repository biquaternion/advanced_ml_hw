{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prettytable\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained baseline models\n",
    "\n",
    "# !pip install gdown\n",
    "## 26 MB\n",
    "# !gdown https://drive.google.com/uc?id=19w7yO0-14U5BoVu0cNBj2UvQjYuszCgP\n",
    "## 7 MB\n",
    "# !gdown https://drive.google.com/uc?id=1yQcsfiOb8v2gfH2EwcFxEzEOq-NQaShg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_data(verbose=True):\n",
    "    print('read data')\n",
    "    players = pd.read_pickle('chgk/players.pkl')\n",
    "    print(f'players: {len(players.items())}')\n",
    "    tournaments = pd.read_pickle('chgk/tournaments.pkl')\n",
    "    print(f'tournaments: {len(tournaments.items())}')\n",
    "    results = pd.read_pickle('chgk/results.pkl')\n",
    "    print(f'results: {len(results.items())}')\n",
    "    return players, tournaments, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players, tournaments, results = read_all_data()\n",
    "players_ = players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tournaments[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(src):\n",
    "    keys = set()\n",
    "    for id in src:\n",
    "        for subj in src[id]:\n",
    "            keys.add(subj)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_keys(players))\n",
    "print(get_keys(tournaments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 - train; 2020 - test\n",
    "def split_train_test():\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "    for t_id in tournaments:\n",
    "        dateStart = datetime.fromisoformat(tournaments[t_id]['dateStart'])\n",
    "        if dateStart.date().year == 2019:\n",
    "            train_ids.append(t_id) # tournaments[t_id]\n",
    "        if dateStart.date().year == 2020:\n",
    "            test_ids.append(t_id) # tournaments[t_id]\n",
    "    return train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = split_train_test()\n",
    "len(train_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_id(src, ids):\n",
    "    return {i: src[i] for i in ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tournaments = filter_by_id(tournaments, train_ids)\n",
    "test_tournaments = filter_by_id(tournaments, test_ids)\n",
    "train_results = filter_by_id(results, train_ids)\n",
    "test_results = filter_by_id(results, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tournaments with constant mask len\n",
    "\n",
    "def filter_results_by_mask_len(src):\n",
    "    result = {}\n",
    "    for t_id in tqdm(src):\n",
    "        masks_lens = set()\n",
    "        for k in src[t_id]:\n",
    "            if 'mask' not in k:\n",
    "                continue\n",
    "            if k['mask'] is not None:\n",
    "                masks_lens.add(len(k['mask']))\n",
    "            else:\n",
    "                continue\n",
    "        masks_len = len(list(masks_lens))\n",
    "        if masks_len != 1:\n",
    "            continue\n",
    "        result[t_id] = t_id\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = filter_results_by_mask_len(train_results)\n",
    "test_ids = filter_results_by_mask_len(test_results)\n",
    "len(train_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tournaments = filter_by_id(tournaments, train_ids)\n",
    "test_tournaments = filter_by_id(tournaments, test_ids)\n",
    "train_results = filter_by_id(results, train_ids)\n",
    "test_results = filter_by_id(results, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_results[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_ids(src):\n",
    "    result = set()\n",
    "    for t_id in tqdm(src):\n",
    "        tournament = src[t_id]\n",
    "        for team in tournament:\n",
    "            if 'teamMembers' not in team:\n",
    "                print(f'missed teamMembers in tournament: {t_id}\\nteam: {team}')\n",
    "                continue\n",
    "            for member in team['teamMembers']:\n",
    "                if 'player' not in member:\n",
    "                    print(f'missed player in tournament: {t_id}')\n",
    "                    print(f'team: {team[\"team\"] if \"team\" in team else \"Unknown\"}')\n",
    "                    print(f'member: {member}')\n",
    "                    continue\n",
    "                if 'id' not in member['player']:\n",
    "                    print(f'missed id in tournament: {t_id}, team: {team}, member: {member}')\n",
    "                    continue\n",
    "                result.add(member['player']['id'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_players = get_player_ids(train_results)\n",
    "test_players = get_player_ids(test_results)\n",
    "\n",
    "print(f'{len(train_players)} players in train,\\t{len(test_players)} players in test')\n",
    "print(f'number of players involved either in train or in test:  {len(train_players.union(test_players))}')\n",
    "print(f'number of players in train, which are not in test:      {len(train_players - test_players)}')\n",
    "print(f'number of players in test, which are not in train:      {len(test_players - train_players)}')\n",
    "print(f'number of players, which are both in train and in test: {len(test_players.intersection(train_players))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_questions(src):\n",
    "    result = 0\n",
    "    for t_id in src:\n",
    "        tournament = src[t_id]\n",
    "        team = tournament[0]\n",
    "        if not 'mask' in team:\n",
    "            print(f'No mask in \\n{team[\"team\"]} in tournament {t_id}')\n",
    "            continue\n",
    "        if team['mask'] is None:\n",
    "            print(f'None mask in \\n{team[\"team\"]} in tournament {t_id}')\n",
    "            continue\n",
    "        result += len(team['mask'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_questions_train = get_number_of_questions(train_results)\n",
    "number_of_questions_test = get_number_of_questions(test_results)\n",
    "number_of_questions_train, number_of_questions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем предположение, что каждый игрок в команде отвечает так же как и вся команда (т.е. соотносим игроков с вопросами, забыв про команды)  \n",
    "Будем обучать логистическую регрессию  \n",
    "$$\n",
    "y=\\sigma(W*x + b)\n",
    "$$  \n",
    "$$ x $$ - пары (игрок, вопрос)  \n",
    "$$ y $$ - правильно или нет ответил игрок на вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таблица \"Игрок\"-\"Вопрос\"-\"Ответ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_players_qa(src_players, src_results):\n",
    "    answers = {k: [] for k in src_players}\n",
    "    questions = {k: [] for k in src_players}\n",
    "    q_id = 0;\n",
    "    for t_id in tqdm(src_results):\n",
    "        tournament = src_results[t_id]\n",
    "        for team in tournament:\n",
    "            if team['mask'] is not None:\n",
    "                for a in team['mask']:\n",
    "                    for p in team['teamMembers']:\n",
    "                        answers[p['player']['id']].append(1 if a == '1' else 0)\n",
    "                        questions[p['player']['id']].append(q_id)\n",
    "                    q_id += 1\n",
    "#                 q_id -= len(team['mask'])\n",
    "    return answers, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_players_answers, train_players_questions = get_players_qa(train_players, train_results)\n",
    "test_players_answers, test_players_questions = get_players_qa(test_players, test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa_table(questions, answers):\n",
    "    result = []\n",
    "    for pq, pa in tqdm(zip(questions.items(), answers.items())):\n",
    "#         set_trace()\n",
    "        if pa[0] != pq[0]:\n",
    "            print(f'error in question {pq}, and aswer {pa}')\n",
    "            continue\n",
    "        if pa[0] not in players:\n",
    "            print(f'no {pa[0]} in players')\n",
    "        if len(pa[1]) != len(pq[1]):\n",
    "#             set_trace()\n",
    "            print(f'error in question {pq}: len {len(pq[1])}, and aswer {pa}, len {len(pa[1])}')\n",
    "            continue\n",
    "        for q, a in zip(pq[1], pa[1]):\n",
    "            result.append([pa[0], q, a])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qa_table = get_qa_table(train_players_questions, train_players_answers)\n",
    "test_qa_table = get_qa_table(test_players_questions, test_players_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_qa_table), len(test_qa_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_qa_table, columns=['players', 'questions', 'answers'])\n",
    "test_df = pd.DataFrame(test_qa_table, columns=['players', 'questions', 'answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_x, train_df_y = train_df[['players', 'questions']], train_df['answers']\n",
    "test_df_x, test_df_y = test_df[['players', 'questions']], test_df['answers']\n",
    "# train_df_x, train_df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv')\n",
    "test_df.to_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique_players = pd.unique(train_df_x['players'])\n",
    "train_unique_questions = pd.unique(train_df_x['questions'])\n",
    "# train_unique_players, train_unique_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_players = pd.unique(test_df_x['players'])\n",
    "test_unique_questions = pd.unique(test_df_x['questions'])\n",
    "# test_unique_players, test_unique_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we train model\n",
    "\n",
    "def fit_model(df_x, df_y):\n",
    "    players_onehot = OneHotEncoder().fit(df_x).transform(df_x)\n",
    "    rating_model = LogisticRegression(verbose=True, max_iter=5000, n_jobs=6).fit(players_onehot, df_y)\n",
    "    return rating_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train (takes 30 mins)\n",
    "# to skip train use pretrained models\n",
    "\n",
    "train_baseline_model_filename = 'train_baseline_model.dat'\n",
    "test_baseline_model_filename = 'test_baseline_model.dat'\n",
    "if not Path(train_baseline_model_filename).exists():\n",
    "    print('train logistic regression on train data')\n",
    "    train_rating_model = fit_model(train_df_x, train_df_y)\n",
    "    print(f'save trained model to {test_baseline_model_filename}')\n",
    "    with open(train_baseline_model_filename, 'wb') as f:\n",
    "        pickle.dump(train_rating_model, f)\n",
    "else:\n",
    "    print('load trained baseline model')\n",
    "    with open(train_baseline_model_filename, 'rb') as f:\n",
    "        train_rating_model = pickle.load(f)\n",
    "    \n",
    "if not Path(test_baseline_model_filename).exists():\n",
    "    print('train logistic regression on test data')\n",
    "    test_rating_model = fit_model(test_df_x, test_df_y)\n",
    "    print(f'save trained model to {test_baseline_model_filename}')\n",
    "    with open(test_baseline_model_filename, 'wb') as f:\n",
    "        pickle.dump(test_rating_model, f)\n",
    "else:\n",
    "    print('load trained baseline model')\n",
    "    with open(test_baseline_model_filename, 'rb') as f:\n",
    "        test_rating_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_model.coef_, train_rating_model.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rating_model.coef_, test_rating_model.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique_players, test_unique_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_players_questions_df(rating_model, unique_players, unique_questions):\n",
    "    # players\n",
    "    rating_players = rating_model.coef_[0][:unique_players.size]\n",
    "    rating_players_df = pd.DataFrame(np.sort(unique_players), columns=['player_id'])\n",
    "    rating_players_df['rating'] = rating_players\n",
    "    best_player_rating, worst_player_rating = rating_players.max(), rating_players.min()\n",
    "    b = worst_player_rating\n",
    "    k = (best_player_rating - b) / 1.0\n",
    "    rating_players_df['norm_rating'] = (rating_players - b) / k\n",
    "    # questions\n",
    "    rating_questions = rating_model.coef_[0][unique_players.size:]\n",
    "    rating_questions_df = pd.DataFrame(np.sort(unique_questions), columns=['question_id'])\n",
    "    rating_questions_df['rating'] = rating_questions\n",
    "    return rating_players_df, rating_questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_players_df, train_rating_questions_df = get_rating_players_questions_df(train_rating_model,\n",
    "                                                                                     train_unique_players,\n",
    "                                                                                     train_unique_questions)\n",
    "print('train')\n",
    "print('players')\n",
    "print(train_rating_players_df)\n",
    "print('questions')\n",
    "print(train_rating_questions_df)\n",
    "\n",
    "test_rating_players_df, test_rating_questions_df = get_rating_players_questions_df(test_rating_model,\n",
    "                                                                                   test_unique_players,\n",
    "                                                                                   test_unique_questions)\n",
    "print('test')\n",
    "print('players')\n",
    "print(test_rating_players_df)\n",
    "print('questions')\n",
    "print(test_rating_questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_players_rating(rating_df, topn=100, player_surname=None, player_name=None):\n",
    "    p_table = PrettyTable()\n",
    "    p_table.field_names = [\"pos\", \"id\", \"rating\", \"player name\"]\n",
    "\n",
    "    for i, row in enumerate(rating_df.sort_values(by='rating', ascending=False).iterrows()):\n",
    "        up_id, (p_id, p_rating, p_norm_rating) = row\n",
    "        s, n, pat = players[int(p_id)][\"surname\"], players[int(p_id)][\"name\"], players[int(p_id)][\"patronymic\"]\n",
    "        if i >= topn:\n",
    "            break\n",
    "        p_name = f'{s} {n} {pat}'\n",
    "\n",
    "        p_table.add_row([i, int(p_id), f'{p_norm_rating:.4f}', p_name])\n",
    "    print(p_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2019 (train)')\n",
    "print_players_rating(train_rating_players_df, topn=10)\n",
    "print('2020 (test)')\n",
    "print_players_rating(test_rating_players_df, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_rating_by_name(rating_df, surname, name=None):\n",
    "    if not isinstance(surname, list):\n",
    "        surname = [surname]\n",
    "    p_table = PrettyTable()\n",
    "    p_table.field_names = [\"pos\", \"id\", \"rating\", \"player name\"]\n",
    "    result = []\n",
    "    for i, row in tqdm(enumerate(rating_df.sort_values(by='rating', ascending=False).iterrows())):\n",
    "        up_id, (p_id, p_rating, p_norm_rating) = row\n",
    "        s, n, pat = players[int(p_id)][\"surname\"], players[int(p_id)][\"name\"], players[int(p_id)][\"patronymic\"]\n",
    "        p_name = f'{s} {n} {pat}'\n",
    "        if s in surname:\n",
    "            if name is None:\n",
    "                result.append([i, int(p_id), f'{p_norm_rating:.4f}', p_name])\n",
    "                p_table.add_row(result[-1])\n",
    "            elif name == n:\n",
    "                result.append([i, int(p_id), f'{p_norm_rating:.4f}', p_name])\n",
    "                p_table.add_row(result[-1])\n",
    "    print(p_table)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = [\"Иванов\"]\n",
    "print(2019)\n",
    "_ = get_player_rating_by_name(train_rating_players_df, probe)\n",
    "print(2020)\n",
    "_ = get_player_rating_by_name(test_rating_players_df, probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# !pip uninstall prettytable\n",
    "# !pip uninstall gdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
